from tempfile import NamedTemporaryFile
from urllib.parse import urlparse

from ecoindex.cli.crawl import EcoindexSpider
from ecoindex.models import WindowSize
from pydantic import AnyHttpUrl, validate_call
from pydantic.types import FilePath
from scrapy.crawler import CrawlerProcess


@validate_call
def validate_list_of_urls(urls: list[AnyHttpUrl]) -> tuple[AnyHttpUrl]:
    result = set()

    for url in urls:
        splitted_url = str(url).split("?")
        result.add(splitted_url[0])

    return result


@validate_call
def get_urls_from_file(urls_file: FilePath) -> list[AnyHttpUrl]:
    with open(urls_file) as fp:
        urls_from_file = set()

        for url in fp.readlines():
            url = url.strip()

            if url:
                urls_from_file.add(url)

    return validate_list_of_urls(urls_from_file)


def get_urls_recursive(main_url: str) -> tuple[str]:
    parsed_url = urlparse(main_url)
    domain = parsed_url.netloc
    main_url = f"{parsed_url.scheme}://{domain}"
    process = CrawlerProcess()

    with NamedTemporaryFile(mode="w+t") as temp_file:
        process.crawl(
            crawler_or_spidercls=EcoindexSpider,
            allowed_domains=[domain],
            start_urls=[main_url],
            temp_file=temp_file,
        )
        process.start()
        temp_file.seek(0)
        urls = temp_file.readlines()

    return validate_list_of_urls(urls)


@validate_call
def get_url_from_args(urls_arg: list[AnyHttpUrl]) -> set[AnyHttpUrl]:
    urls_from_args = set()
    for url in urls_arg:
        urls_from_args.add(url)

    return urls_from_args


def get_window_sizes_from_args(window_sizes: list[str]) -> list[WindowSize]:
    result = []
    errors = ""
    for window_size in window_sizes:
        try:
            width, height = window_size.split(",")
            result.append(WindowSize(width=int(width), height=int(height)))
        except ValueError:
            errors += f"ðŸ”¥ `{window_size}` is not a valid window size. Must be of type `1920,1080`\n"

    if errors:
        raise ValueError(errors)

    return result


def get_file_prefix_input_file_logger_file(
    urls: list[AnyHttpUrl],
    urls_file: str | None = None,
    tmp_folder: str = "/tmp/ecoindex-cli",
) -> tuple[str, str, str]:
    """
    Returns file prefix, input file and logger file based on provided urls
    and provider method: If this is based on an existing csv file, we take
    the name of the file, else, we take the first provided url's domain
    """
    if urls_file:
        file_prefix = urls_file.split("/")[-1]
        input_file = urls_file
    else:
        first_url = str(next(iter(urls)))
        file_prefix = urlparse(first_url).netloc
        input_file = f"{tmp_folder}/input/{file_prefix}.csv"

    return (file_prefix, input_file, f"{tmp_folder}/logs/{file_prefix}.log")
